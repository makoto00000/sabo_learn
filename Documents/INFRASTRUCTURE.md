# インフラ

## 構成図

![インフラ構成図](https://github.com/makoto00000/sabo_learn/assets/65654634/de75ef3c-dbcb-40e3-8c32-14eed87d46a6)

## 選定理由

### Amplify

Next.jsを使った開発のため、Vercelへのデプロイも考えましたが20ドル/月かかります。ユーザー数が少ないうちはAWSを使う方が料金を抑えられるためAWSのサービスを使用しました。Cloudfront + S3を自分で構築する方法もありますが、実装期間が限られていることもあり、簡便にgithubと連携してプルリクベースでデプロイもできるAmplifyを選択しました。

### Certificate Manager

https通信を行うため、SSL/TLS証明書を管理しています。

### Route53

お名前.comで取得したドメインをルーティングさせています。フロント用とバックエンド用にサブドメインを設定しました。

### Application Load Barancer

障害時やオートスケール時の入力トラフィックを適切なコンテナに分散させています。

### ECR/ECS(Fargate)

Dockerベースでの開発を行ったため、ECRを採用しました。ECSを選択することで、EC2に比べてインフラの管理がラクになるのが最大のメリットです。もう一つの採用理由はECSを使った構成をやったことがないためというのがあります。運用コストのことを考えると、Fargateを採用することで、パフォーマンスに対するコンピューティングコストは、ECS on EC2に比べると割高になってしまうため、今後の検討が必要ではある部分だと考えています。

## CI/CD

### フロントエンド

Amplifyをgithubと連携しており、mainブランチにマージすると自動でデプロイが実行されます。

### バックエンド

Github Actionsを導入しており、プルリクエスト時にテストが実行されます。テストにはRspecを導入しており、基本的なバックエンドの単体テスト、結合テストを記述しました。またRubocopも実行されるようになっており、コードの品質を担保しています。
Backend（Rails）とSocket Serverは、それぞれのディレクトリの変更を検知し、Github Actionsで、ECRへのpushおよび、ECSへのデプロイまで自動化させました。

### シグナリングサーバー

WebRTCのSDP交換のためのシグナリングサーバを、Node.js × Socket.ioで実装しました。最大4人までのP2Pを実現するためには、room機能が必要でした。Socket.ioはそのroom機能を提供しており、またドキュメントも整っており、難しい記述を必要としない点から採用しました。

## 監視

Next.jsにsentryを導入し、エラー発生時はメール通知としました。（Slack通知は有料プランへの登録が必要だったため）backend、socket server、RDSはそれぞれCloudWatchでエラーを監視し、Slackに通知されるよう設定しました。
